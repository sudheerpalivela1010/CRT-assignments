{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e6913d0-a1c0-490a-9a83-fd9131b037bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: \n",
      "Natural Language Processing is a field of artificial intelligence. It focuses on making computers understand and process human languages.\n",
      "This includes tasks like tokenization, stemming, lemmatization, and much more.\n",
      "\n",
      "\n",
      "Tokens: ['Natural', 'Language', 'Processing', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', '.', 'It', 'focuses', 'on', 'making', 'computers', 'understand', 'and', 'process', 'human', 'languages', '.', 'This', 'includes', 'tasks', 'like', 'tokenization', ',', 'stemming', ',', 'lemmatization', ',', 'and', 'much', 'more', '.']\n",
      "\n",
      "Filtered Tokens (Stopwords removed): ['Natural', 'Language', 'Processing', 'field', 'artificial', 'intelligence', 'focuses', 'making', 'computers', 'understand', 'process', 'human', 'languages', 'includes', 'tasks', 'like', 'tokenization', 'stemming', 'lemmatization', 'much']\n",
      "\n",
      "Stemmed Tokens: ['natur', 'languag', 'process', 'field', 'artifici', 'intellig', 'focus', 'make', 'comput', 'understand', 'process', 'human', 'languag', 'includ', 'task', 'like', 'token', 'stem', 'lemmat', 'much']\n",
      "\n",
      "Lemmatized Tokens: ['Natural', 'Language', 'Processing', 'field', 'artificial', 'intelligence', 'focus', 'making', 'computer', 'understand', 'process', 'human', 'language', 'includes', 'task', 'like', 'tokenization', 'stemming', 'lemmatization', 'much']\n",
      "\n",
      "Gensim Dictionary: {'Language': 0, 'Natural': 1, 'Processing': 2, 'artificial': 3, 'computer': 4, 'field': 5, 'focus': 6, 'human': 7, 'includes': 8, 'intelligence': 9, 'language': 10, 'lemmatization': 11, 'like': 12, 'making': 13, 'much': 14, 'process': 15, 'stemming': 16, 'task': 17, 'tokenization': 18, 'understand': 19}\n",
      "\n",
      "Gensim Corpus: [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/varikuntlasaimanoj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/varikuntlasaimanoj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/varikuntlasaimanoj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"\"\"\n",
    "Natural Language Processing is a field of artificial intelligence. It focuses on making computers understand and process human languages.\n",
    "This includes tasks like tokenization, stemming, lemmatization, and much more.\n",
    "\"\"\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalnum()]\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "dictionary = corpora.Dictionary([lemmatized_tokens])\n",
    "corpus = [dictionary.doc2bow(lemmatized_tokens)]\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"\\nTokens:\", tokens)\n",
    "print(\"\\nFiltered Tokens (Stopwords removed):\", filtered_tokens)\n",
    "print(\"\\nStemmed Tokens:\", stemmed_tokens)\n",
    "print(\"\\nLemmatized Tokens:\", lemmatized_tokens)\n",
    "print(\"\\nGensim Dictionary:\", dictionary.token2id)\n",
    "print(\"\\nGensim Corpus:\", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67627e9-55e2-4300-ab5d-632094f1c972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
